{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import os as os\n",
    "import pylab as plt\n",
    "import jax\n",
    "import time\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from scipy.linalg import fractional_matrix_power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Rosenbrock function with different optimizers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rosenbrock function \n",
    "$$\n",
    "F(x_1, x_2) = 100 \\times (x_2 - x_1^2)^2 + (1-x_1)^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that (1,1) is the global minimum with minimum value 0. We will try to minimize this function with an initial guess (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def rosenbrock(x): \n",
    "    x1, x2 = x\n",
    "    return 100 * (x2 - x1 ** 2) ** 2 + (1 - x1) ** 2\n",
    "\n",
    "value_and_grad_rosenbrock = jax.jit(jax.value_and_grad(rosenbrock))\n",
    "\n",
    "x_init = np.array([0., 0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will minimize the function using \n",
    "1. Naive gradient descent \n",
    "2. Gradient descent with ADAM optimizer \n",
    "3. Gradient descent with RMSprop optimizer\n",
    "4. Gradient descent with Shampoo optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Final Loss</th>\n",
       "      <th>Loss Variance</th>\n",
       "      <th>Time Taken (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.060175</td>\n",
       "      <td>0.046657</td>\n",
       "      <td>0.057846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.147262</td>\n",
       "      <td>0.048067</td>\n",
       "      <td>0.085769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.280596</td>\n",
       "      <td>0.039403</td>\n",
       "      <td>0.081781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.447978</td>\n",
       "      <td>0.024860</td>\n",
       "      <td>0.113697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.619794</td>\n",
       "      <td>0.012051</td>\n",
       "      <td>0.137633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Learning Rate  Final Loss  Loss Variance  Time Taken (s)\n",
       "12       0.001438    0.060175       0.046657        0.057846\n",
       "11       0.000785    0.147262       0.048067        0.085769\n",
       "10       0.000428    0.280596       0.039403        0.081781\n",
       "9        0.000234    0.447978       0.024860        0.113697\n",
       "8        0.000127    0.619794       0.012051        0.137633"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates = np.logspace(-6, -1, num = 20)  # Learning rates from 1e-6 to 1\n",
    "num_iterations = 1000\n",
    "\n",
    "results = pd.DataFrame(columns=['Learning Rate', 'Final Loss', 'Loss Variance', 'Time Taken (s)'])\n",
    "\n",
    "for lr in learning_rates:\n",
    "    x = np.copy(x_init)\n",
    "    losses = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        current_loss, grad = value_and_grad_rosenbrock(x)\n",
    "        losses.append(current_loss)\n",
    "        \n",
    "        x -= lr * grad  # Gradient descent update\n",
    "    \n",
    "    end_time = time.time() \n",
    "\n",
    "    final_loss = losses[-1]\n",
    "    loss_variance = np.var(losses)\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    # Store results in a DataFrame\n",
    "    results = pd.concat([results, pd.DataFrame({\n",
    "        'Learning Rate': [lr],\n",
    "        'Final Loss': [final_loss],\n",
    "        'Loss Variance': [loss_variance],\n",
    "        'Time Taken (s)': [time_taken]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "results['Final Loss'] = results['Final Loss'].astype(float)\n",
    "results = results.sort_values(by = 'Final Loss')\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ADAM Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Beta1</th>\n",
       "      <th>Beta2</th>\n",
       "      <th>Final Loss</th>\n",
       "      <th>Loss Variance</th>\n",
       "      <th>Time Taken (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.054556</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>0.430824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.008859</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.999</td>\n",
       "      <td>2.2737368e-13</td>\n",
       "      <td>0.022579</td>\n",
       "      <td>0.312198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.008859</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.4352963e-12</td>\n",
       "      <td>0.022999</td>\n",
       "      <td>0.299195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.004833</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.999</td>\n",
       "      <td>2.689049e-11</td>\n",
       "      <td>0.038032</td>\n",
       "      <td>0.314193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.029764</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.999</td>\n",
       "      <td>5.685621e-10</td>\n",
       "      <td>0.008537</td>\n",
       "      <td>0.530584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Learning Rate  Beta1  Beta2     Final Loss  Loss Variance  Time Taken (s)\n",
       "168       0.054556   0.95  0.999            0.0       0.007362        0.430824\n",
       "141       0.008859   0.95  0.999  2.2737368e-13       0.022579        0.312198\n",
       "135       0.008859   0.90  0.999  1.4352963e-12       0.022999        0.299195\n",
       "132       0.004833   0.95  0.999   2.689049e-11       0.038032        0.314193\n",
       "159       0.029764   0.95  0.999   5.685621e-10       0.008537        0.530584"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Range of values that we are trying \n",
    "learning_rates = np.logspace(-6, -1, num=20)  \n",
    "beta1_vals = [0.9, 0.85, 0.95]\n",
    "beta2_vals = [0.999, 0.99, 0.95]\n",
    "max_iter = 1000\n",
    "\n",
    "# Initialize results DataFrame\n",
    "results = pd.DataFrame(columns=['Learning Rate', 'Beta1', 'Beta2', 'Final Loss', 'Loss Variance', 'Time Taken (s)'])\n",
    "\n",
    "# Adam optimizer parameters\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Perform grid search over learning rate and beta values\n",
    "for lr, beta1, beta2 in product(learning_rates, beta1_vals, beta2_vals):\n",
    "    x = np.copy(x_init)  # Initialize x\n",
    "    loss_hist = []  # Store loss history for variance calculation\n",
    "\n",
    "    # Initialize Adam-specific variables\n",
    "    g = np.zeros(2)\n",
    "    v = np.zeros(2)\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Adam optimization loop\n",
    "    for it in range(max_iter):\n",
    "        current_loss, grad = value_and_grad_rosenbrock(x)  # Calculate loss and gradient\n",
    "        loss_hist.append(current_loss)\n",
    "\n",
    "        # Update running averages for gradient and squared gradient\n",
    "        g = beta1 * g + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "\n",
    "        # Bias correction\n",
    "        g_corrected = g / (1 - beta1 ** (it + 1))\n",
    "        v_corrected = v / (1 - beta2 ** (it + 1))\n",
    "\n",
    "        # Compute normalized gradient for Adam update\n",
    "        grad_normalized = g_corrected / (np.sqrt(v_corrected) + epsilon)\n",
    "\n",
    "        # Update x\n",
    "        x -= lr * grad_normalized\n",
    "\n",
    "    # End timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate metrics for this combination of parameters\n",
    "    final_loss = loss_hist[-1]\n",
    "    loss_variance = np.var(loss_hist)\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    # Store results in DataFrame\n",
    "    results = pd.concat([results, pd.DataFrame({\n",
    "        'Learning Rate': [lr],\n",
    "        'Beta1': [beta1],\n",
    "        'Beta2': [beta2],\n",
    "        'Final Loss': [final_loss],\n",
    "        'Loss Variance': [loss_variance],\n",
    "        'Time Taken (s)': [time_taken]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "results = results.sort_values(by='Final Loss')\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RMSprop Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Final Loss</th>\n",
       "      <th>Loss Variance</th>\n",
       "      <th>Time Taken (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.035552815</td>\n",
       "      <td>0.026944</td>\n",
       "      <td>0.269815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.04610299</td>\n",
       "      <td>0.025752</td>\n",
       "      <td>0.252068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.073455006</td>\n",
       "      <td>0.023495</td>\n",
       "      <td>0.256473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.22157194</td>\n",
       "      <td>0.013768</td>\n",
       "      <td>0.246529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.46794412</td>\n",
       "      <td>0.023678</td>\n",
       "      <td>0.269163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Learning Rate  Beta   Final Loss  Loss Variance  Time Taken (s)\n",
       "12       0.005623  0.70  0.035552815       0.026944        0.269815\n",
       "13       0.005623  0.80   0.04610299       0.025752        0.252068\n",
       "14       0.005623  0.90  0.073455006       0.023495        0.256473\n",
       "15       0.005623  0.99   0.22157194       0.013768        0.246529\n",
       "8        0.000316  0.70   0.46794412       0.023678        0.269163"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient descent for RMSprop\n",
    "# Gradient descent with RMSProp\n",
    "learning_rates = np.logspace(-6, -1, num=5)  # Test learning rates from 1e-6 to 0.1\n",
    "betas = [0.7, 0.8, 0.9, 0.99]  # Test a few values for beta\n",
    "\n",
    "# Initialize a DataFrame to store the results\n",
    "results = pd.DataFrame(columns=['Learning Rate', 'Beta', 'Final Loss', 'Loss Variance', 'Time Taken (s)'])\n",
    "\n",
    "# Parameters\n",
    "epsilon = 1e-8\n",
    "max_iter = 1000\n",
    "\n",
    "for lr, beta in product(learning_rates, betas):\n",
    "    x = np.copy(x_init)\n",
    "    losses = []\n",
    "    v = np.zeros(2)  # Running average of squared gradient, initialized to 0\n",
    "\n",
    "    # Start timer for this combination of hyperparameters\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run gradient descent with RMSProp\n",
    "    for it in range(max_iter): \n",
    "        current_loss, grad = value_and_grad_rosenbrock(x)\n",
    "        losses.append(current_loss)\n",
    "\n",
    "        # Update the running average of the squared gradient\n",
    "        v = (1 - beta) * v + beta * (grad ** 2)\n",
    "\n",
    "        # Compute the normalized gradient\n",
    "        grad_normalized = grad / (np.sqrt(v) + epsilon)\n",
    "\n",
    "        # Update parameters\n",
    "        x = x - lr * grad_normalized\n",
    "\n",
    "    # End timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Collect results\n",
    "    final_loss = losses[-1]\n",
    "    loss_variance = np.var(losses)\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    # Append results to DataFrame\n",
    "    results = pd.concat([results, pd.DataFrame({\n",
    "        'Learning Rate': [lr],\n",
    "        'Beta': [beta],\n",
    "        'Final Loss': [final_loss],\n",
    "        'Loss Variance': [loss_variance],\n",
    "        'Time Taken (s)': [time_taken]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "results = results.sort_values(by='Final Loss')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Shampoo Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Final Loss</th>\n",
       "      <th>Loss Variance</th>\n",
       "      <th>Time Taken (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.016199157</td>\n",
       "      <td>0.042131</td>\n",
       "      <td>0.528066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.22621474</td>\n",
       "      <td>0.045951</td>\n",
       "      <td>0.537434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.6761303</td>\n",
       "      <td>0.008754</td>\n",
       "      <td>0.537843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.9175868</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.618315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.98162365</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.883934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Learning Rate   Final Loss  Loss Variance  Time Taken (s)\n",
       "5       0.002154  0.016199157       0.042131        0.528066\n",
       "4       0.000464   0.22621474       0.045951        0.537434\n",
       "3       0.000100    0.6761303       0.008754        0.537843\n",
       "2       0.000022    0.9175868       0.000567        0.618315\n",
       "1       0.000005   0.98162365       0.000028        0.883934"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates = np.logspace(-6, 0, num = 10)  # Learning rates from 1e-6 to 0.1\n",
    "max_iter = 1000\n",
    "\n",
    "# Initialize a DataFrame to store the results\n",
    "results = pd.DataFrame(columns=['Learning Rate', 'Final Loss', 'Loss Variance', 'Time Taken (s)'])  \n",
    "epsilon = 1e-2\n",
    "max_iter = 1000\n",
    "\n",
    "for lr in learning_rates:\n",
    "    loss_hist = []\n",
    "    x = np.copy(x_init)\n",
    "\n",
    "    # Initialize Shampoo-specific matrices\n",
    "    L = np.zeros((2, 2))  \n",
    "    R = np.zeros(1)  \n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        current_loss, grad = value_and_grad_rosenbrock(x)\n",
    "        loss_hist.append(current_loss)\n",
    "\n",
    "        grad = np.clip(grad, -1e3, 1e3)  # Adjust the clip range as needed\n",
    "        \n",
    "        # Update the gradient accumulation matrices\n",
    "        L += np.outer(grad, grad) + epsilon * np.eye(2) # Outer product for Shampoo update\n",
    "        R += np.dot(grad, grad) + epsilon # Inner product for Shampoo update\n",
    "        \n",
    "        # Compute the matrix root raised to the -1/4 power with epsilon for numerical stability\n",
    "        L_root = fractional_matrix_power(L, 0.25)\n",
    "        R_root = R ** (-1/4)\n",
    "\n",
    "        # Use np.linalg.solve instead of np.linalg.inv for L_root and R_root\n",
    "        L_inv_root = np.linalg.solve(L_root, np.eye(2))\n",
    "        R_inv_root = 1 / R_root\n",
    "\n",
    "        # Precondition the gradient with L^(-1/4) * grad * R^(-1/4)\n",
    "        preconditioned_grad = L_inv_root @ grad * R_inv_root\n",
    "\n",
    "        # Update parameters with the preconditioned gradient\n",
    "        x = x - lr * preconditioned_grad\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Collect results\n",
    "    final_loss = loss_hist[-1]\n",
    "    loss_variance = np.var(loss_hist)\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    # Append results to DataFrame\n",
    "    results = pd.concat([results, pd.DataFrame({\n",
    "        'Learning Rate': [lr],\n",
    "        'Final Loss': [final_loss],\n",
    "        'Loss Variance': [loss_variance],\n",
    "        'Time Taken (s)': [time_taken]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "results = results.sort_values(by='Final Loss')\n",
    "results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
